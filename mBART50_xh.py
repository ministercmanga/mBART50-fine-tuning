# -*- coding: utf-8 -*-
"""Untitled1_xh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mri8-0fEnxZc0BLH4IXU2p4ZLNDlkLcm
"""

!pip install transformers datasets sacrebleu sentencepiece

from datasets import Dataset
import pandas as pd

# Load your parallel data
with open('/content/drive/MyDrive/English-isiXhosa/English1_cleaned_new.txt', 'r', encoding='utf-8') as f:
    en_lines = f.read().splitlines()

with open('/content/drive/MyDrive/English-isiXhosa/isiXhosa1_cleaned_new.txt', 'r', encoding='utf-8') as f:
    xh_lines = f.read().splitlines()

assert len(en_lines) == len(xh_lines), "Parallel corpora not aligned!"

# Create a DataFrame
data = pd.DataFrame({'translation': [{'en_XX': en, 'xh_ZA': xh} for en, xh in zip(en_lines, xh_lines)]})

# Convert to Hugging Face dataset
dataset = Dataset.from_pandas(data)

# Split into train/test
dataset = dataset.train_test_split(test_size=0.1)

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

model_name = 'facebook/mbart-large-50-many-to-many-mmt'
tokenizer = MBart50TokenizerFast.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name)

# Set source and target languages
tokenizer.src_lang = "en_XX"
target_lang = "xh_ZA"

# Step 1: Flatten translation fields
dataset = dataset.map(lambda x: {'en_XX': x['translation']['en_XX'], 'xh_ZA': x['translation']['xh_ZA']})

# Step 2: Tokenize batched
max_length = 128

def preprocess_batch(batch):
    model_inputs = tokenizer(
        batch["en_XX"],
        max_length=max_length,
        padding="max_length",
        truncation=True
    )

    labels = tokenizer(
        batch["xh_ZA"],
        max_length=max_length,
        padding="max_length",
        truncation=True
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess_batch, batched=True, remove_columns=["en_XX", "xh_ZA"])

!pip install evaluate --quiet

import evaluate
import numpy as np

bleu = evaluate.load("bleu")

def compute_bleu(eval_preds):
    preds, labels = eval_preds

    if isinstance(preds, tuple):
        preds = preds[0]

    # Replace -100 in labels with pad_token_id so they can be decoded
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    # Decode back to strings
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Strip whitespace
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    # Compute BLEU
    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)

    # ✅ Only return BLEU score
    return {"bleu": result["bleu"]}

import torch
torch.cuda.empty_cache()

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq

training_args = Seq2SeqTrainingArguments(
    output_dir="./mbart_xh_model",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=5,
    predict_with_generate=True,
    logging_strategy="steps",
    logging_steps=10,
    generation_max_length=128,
    fp16=True,
    report_to="none"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_bleu
)

trainer.train()

results = trainer.evaluate()
print("✅ BLEU Score:", results["eval_bleu"])

from google.colab import drive
drive.mount('/content/drive')

model.save_pretrained("mbart-xh-finetuned")
tokenizer.save_pretrained("mbart-xh-finetuned")

import torch

def translate(text, model, tokenizer, target_lang="xh_ZA", max_length=128):
    # Set device (GPU if available)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Tokenize and move to device
    inputs = tokenizer(text, return_tensors="pt", max_length=max_length, truncation=True).to(device)

    # Set forced BOS token for the target language
    forced_bos_token_id = tokenizer.lang_code_to_id[target_lang]

    # Generate translation
    with torch.no_grad():
        generated_tokens = model.generate(
            **inputs,
            forced_bos_token_id=forced_bos_token_id,
            max_length=max_length,
            num_beams=5,
            early_stopping=True
        )

    # Decode the output
    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)

source_text = "My family lives in the Eastern Cape province."
print("XHOSA:", translate(source_text, model, tokenizer))